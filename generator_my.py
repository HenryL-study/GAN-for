import tensorflow as tf

class Generator(object):

    def __init__(self, num_emb, batch_size, emb_dim, encoder_num_units, emb_data
                 sequence_length, start_token,
                 learning_rate=0.01, reward_gamma=0.95):
        self.num_emb = num_emb
        self.batch_size = batch_size
        self.emb_dim = emb_dim
        self.encoder_num_units = encoder_num_units
        self.max_sequence_length = sequence_length
        self.start_token = tf.constant([start_token] * self.batch_size, dtype=tf.int32)
        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)
        self.reward_gamma = reward_gamma
        #self.g_params = []
        #self.d_params = []
        #self.temperature = 1.0
        self.grad_clip = 5.0

        self.seq_start_token = None
        self.seq_end_token = None
        self.encode_rnn_size = 50
        self.encode_layer_size = 2
        self.decode_rnn_size = 50
        self.decode_layer_size = 2

        self.expected_reward = tf.Variable(tf.zeros([self.sequence_length]))

        #with tf.variable_scope('generator'):
        self.g_embeddings = tf.Variable(self.init_matrix([self.num_emb, self.emb_dim]))
            #self.g_params.append(self.g_embeddings)
            #self.g_recurrent_unit = self.create_recurrent_unit(self.g_params)  # maps h_tm1 to h_t for generator
            #self.g_output_unit = self.create_output_unit(self.g_params)  # maps h_t to o_t (output token logits)
        
        self.x = tf.placeholder(tf.int32, shape=[self.batch_size, self.sequence_length]) # sequence of tokens generated by generator
        self.rewards = tf.placeholder(tf.float32, shape=[self.batch_size, self.sequence_length]) # get from rollout policy and discriminator
        self.target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length')

        with tf.device("/cpu:0"):
            #self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, self.x), perm=[1, 0, 2])  # seq_length x batch_size x emb_dim
            self.processed_x = tf.nn.embedding_lookup(self.g_embeddings, self.x)
        
        _, encoder_state = get_encoder_layer(self.processed_x, self.encode_rnn_size, self.encode_layer_size)

        training_decoder_output, predicting_decoder_output = decoding_layer(self.decode_layer_size, 
                                                                            self.decode_rnn_size,
                                                                            self.target_sequence_length,
                                                                            self.max_sequence_length,
                                                                            encoder_state, 
                                                                            self.x)
        
        #######################################################################################################
        #  Pre-Training
        #######################################################################################################
        self.g_pretrain_predictions = training_decoder_output.rnn_output
        
        masks = tf.sequence_mask(self.target_sequence_length, self.max_sequence_length, dtype=tf.float32, name='masks')
        self.pretrain_loss = tf.contrib.seq2seq.sequence_loss(
            g_pretrain_predictions,
            self.x,
            masks)
        # training updates
        pretrain_opt = self.g_optimizer(self.learning_rate)

        pre_gradients = pretrain_opt.compute_gradients(pretrain_loss)
        self.pretrain_grad_zip = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in pre_gradients if grad is not None]
        self.pretrain_updates = optimizer.apply_gradients(self.pretrain_grad_zip)

        #######################################################################################################
        #  Unsupervised Training
        #######################################################################################################
        self.g_predictions = predicting_decoder_output.rnn_output
        self.g_loss = -tf.reduce_sum(
            tf.reduce_sum(
                tf.one_hot(tf.to_int32(tf.reshape(self.x, [-1])), self.num_emb, 1.0, 0.0) * tf.log(
                    tf.clip_by_value(tf.reshape(self.g_predictions, [-1, self.num_emb]), 1e-20, 1.0)
                ), 1) * tf.reshape(self.rewards, [-1])
        )

        g_opt = self.g_optimizer(self.learning_rate)
        g_gradients = g_opt.compute_gradients(g_loss)
        self.g_grad_zip = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in g_gradients if grad is not None]
        self.g_updates = optimizer.apply_gradients(self.g_grad_zip)


    def init_matrix(self, shape):
        #return tf.random_normal(shape, stddev=0.1)
        
        #TODO decide start & end
        self.seq_start_token = 0
        self.seq_end_token = ??

        embeddings = tf.get_variable("embeddings", shape=emb_data.shape, initializer=tf.constant_initializer(emb_data), trainable=True)
        return embeddings

    def get_encoder_layer(input_data, rnn_size, num_layers, source_sequence_length):
        
        '''
        Encoder layer

        Args:
        - input_data: a Tensor of shape [batch_size, seq_length, emb_dim]
        - rnn_size: num of hidden states in rnn
        - num_layers: layers of rnn
        - source_sequence_length: seq lenth of each data
        '''
        #rnn cell
        #TODO change to bi-rnn
        #
        # 首先构造单个rnn cell
        # encoder_f_cell = LSTMCell(self.hidden_size)
        # encoder_b_cell = LSTMCell(self.hidden_size)
        # (encoder_fw_outputs, encoder_bw_outputs),
        # (encoder_fw_final_state, encoder_bw_final_state) = \
        #         tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_f_cell,
        #                                             cell_bw=encoder_b_cell,
        #                                             inputs=self.encoder_inputs_embedded,
        #                                             sequence_length=self.encoder_inputs_actual_length,
        #                                             dtype=tf.float32, time_major=True)
        #
        ########################################################################################################
        def get_lstm_cell(rnn_size):
            lstm_cell = tf.contrib.rnn_cell.BasicLSTMCell(encoder_num_units)
            return lstm_cell
        
        cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(rnn_size) for _ in range(num_layers)])

        encoder_output, encoder_state = tf.nn.dynamic_rnn(cell, input_data, 
                                                      sequence_length=source_sequence_length, dtype=tf.float32)

        return encoder_output, encoder_state

    def decoding_layer(num_layers, rnn_size, target_sequence_length, 
                        max_target_sequence_length, encoder_state, decoder_input):
        '''
        构造Decoder层
        
        参数：
        #- target_letter_to_int: target数据的映射表
        #- decoding_embedding_size: embed向量大小
        - num_layers: 堆叠的RNN单元数量
        - rnn_size: RNN单元的隐层结点数量
        - target_sequence_length: target数据序列长度
        - max_target_sequence_length: target数据序列最大长度
        - encoder_state: encoder端编码的状态向量
        - decoder_input: decoder端输入
        '''
        # 1. Embedding
        target_vocab_size = self.num_emb
        # decoder_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))
        decoder_embed_input = tf.nn.embedding_lookup(self.g_embeddings, decoder_input)

        # 2. 构造Decoder中的RNN单元
        def get_decoder_cell(rnn_size):
            decoder_cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))
            return decoder_cell
        cell = tf.contrib.rnn.MultiRNNCell([get_decoder_cell(rnn_size) for _ in range(num_layers)])
        
        # 3. Output全连接层
        output_layer = Dense(target_vocab_size,
                            kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))


        # 4. Training decoder
        with tf.variable_scope("decode"):
            # 得到help对象
            training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input,
                                                                sequence_length=target_sequence_length,
                                                                time_major=False)
            # 构造decoder
            training_decoder = tf.contrib.seq2seq.BasicDecoder(cell,
                                                            training_helper,
                                                            encoder_state,
                                                            output_layer) 
            training_decoder_output, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,
                                                                        impute_finished=True,
                                                                        maximum_iterations=max_target_sequence_length)
        # 5. Predicting decoder
        # 与training共享参数
        with tf.variable_scope("decode", reuse=True):
            # 创建一个常量tensor并复制为batch_size的大小
            start_tokens = tf.tile(tf.constant([self.seq_start_token], dtype=tf.int32), [batch_size], 
                                name='start_tokens')
            predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.g_embeddings,
                                                                    start_tokens,
                                                                    self.seq_end_token)
            predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell,
                                                            predicting_helper,
                                                            encoder_state,
                                                            output_layer)
            predicting_decoder_output, _ = tf.contrib.seq2seq.dynamic_decode(predicting_decoder,
                                                                impute_finished=True,
                                                                maximum_iterations=max_target_sequence_length)
        
        return training_decoder_output, predicting_decoder_output

    def generate(self, sess):
        outputs = sess.run(self.g_predictions)
        return outputs

    def pretrain_step(self, sess, x):
        outputs = sess.run([self.pretrain_grad_zip, self.pretrain_updates], feed_dict={self.x: x})
        return outputs

    def g_optimizer(self, *args, **kwargs):
        return tf.train.AdamOptimizer(*args, **kwargs)

    